# NeurodivergentHelper â€“ Lightweight CPU Version
* Updated: Aug 25, 2025

**Addressed to:** Chuck Merriman and Kyle Schneider  
**Website Integration:** https://neurodivergentexperiences.com  
**Purpose:** Free CPU-optimized version of NeurodivergentHelper designed for local testing and deployment. A paid version with extended capabilities, GPU support, advanced memory management, and premium modules is under development.

The CPU version is intended for self-hosting on Windows, macOS, or Linux. GPU-accelerated features are only available in Linux environments with a CUDA-compatible NVIDIA GPU.

---

## Overview

NeurodivergentHelper is an advanced emotional intelligence and linguistic support system tailored to neurodivergent individuals. It leverages context-aware processing, trauma-informed design, and sensory-aware interactions to provide safe, responsive, and adaptive support for complex emotional, social, and cognitive needs.

**Key Capabilities:**

- **Cognitive and Emotional Support:** Offers reframing techniques, emotional regulation guidance, mindfulness exercises, and strategies for internal system harmony.  
- **Neurotype-Specific Assistance:** Supports autistic, ADHD, SPD, and trauma-affected users with sensitive handling of emotional cues.  
- **Adaptive Session Memory:** Tracks the last 5 user interactions to ensure context-aware responses.  
- **Embeddable Chat Interface:** Easily integrated into websites via the `/embed` endpoint.  
- **FastAPI Endpoints:** Provides flexible API endpoints for custom frontend development and integrations.  
- **Open-Ended Extensibility:** Can be extended for Discord, Replit, and other platforms, with modular overrides and configuration.  

**Alpha Deployments:**

- Poe: [NeurodivergentCalm](https://poe.com/NeurodivergentCalm)  
- Facebook Messenger: [Messenger Bot](https://m.me/1944934973012617?is_ai=1)  

---

## System Requirements

**CPU Version (Lightweight)**

- OS: Windows 10/11, macOS, or Linux  
- RAM: 8 GB minimum, 16 GB recommended  
- Disk: ~2 GB for dependencies, cache, and models  

**GPU Version (Future / Replit)**

- GPU: NVIDIA CUDA-compatible (12.1)  
- VRAM: 6â€“8 GB minimum  
- OS: Linux preferred for full GPU support  

> Note: GPU passthrough on Windows is not supported. Linux with CUDA is required for GPU execution.

---

## File Structure

```
NeurodivergentHelper/
â”œâ”€ app.py                  # FastAPI + Gradio integration
â”œâ”€ models.py               # CPU-optimized model loader
â”œâ”€ utils.py                # Helper functions for session & logging
â”œâ”€ style.css               # Custom CSS for Gradio/embed
â”œâ”€ requirements.txt        # Python dependencies
â”œâ”€ Dockerfile              # Container configuration for CPU execution
â”œâ”€ .gitignore              # Ignore cache, temporary files, and virtual environments
â”œâ”€ NeurodivergentHelper.prompt.yml  # YAML system prompt
â”œâ”€ models.yml              # Optional YAML model configuration
â”œâ”€ README.md               # This documentation
â””â”€ overrides/              # Optional custom app.py or Dockerfile overrides
```

### Key Files

- `app.py` â€“ Entrypoint handling API and Gradio interface. Supports `/query`, `/embed`, and `/docs`.  
- `models.py` â€“ Handles dynamic CPU-optimized model loading, caching, and device management.  
- `utils.py` â€“ Session and logging helpers.  
- `style.css` â€“ Custom styling for the embeddable chat interface.  
- `requirements.txt` â€“ Python dependencies required for CPU-based execution, including `transformers` and `pyyaml`.  
- `Dockerfile` â€“ Docker container setup optimized for CPU deployment with pre-configured environment variables and thread settings.  
- `NeurodivergentHelper.prompt.yml` â€“ YAML-based system prompt configuration.  
- `models.yml` â€“ Optional YAML-based custom model selection.  
- `.gitignore` â€“ Excludes temporary files, caches, virtual environments, and model downloads.  

---

## Website Integration

### 1. Direct iframe Embedding

```
<iframe   
    src="https://your-domain.com/embed"   
    width="600"   
    height="500"  
    frameborder="0"  
    title="NeurodivergentHelper Chat">  
</iframe>
```

### 2. API Integration for Custom Frontends

```
const response = await fetch('/query', {  
    method: 'POST',  
    headers: { 'Content-Type': 'application/json' },  
    body: JSON.stringify({ prompt: userMessage })  
});  
const data = await response.json();  
console.log(data.response);
```

**Available Endpoints:**

* `/` â€“ API status and model information  
* `/embed` â€“ Embeddable chat interface  
* `/query` â€“ Chat API endpoint  
* `/docs` â€“ FastAPI documentation  

---

## Configuration

### Hugging Face Token (Mandatory for Model Downloads)

**Linux / WSL / macOS:**  

```
export HF_TOKEN="your_token_here"
export MODEL_NAME="username/custom-model"  # Optional
```

**Windows PowerShell:**  

```
$env:HF_TOKEN="your_token_here"
$env:MODEL_NAME="username/custom-model"  # Optional
```

**Windows Command Prompt (cmd.exe):**  

```
set HF_TOKEN=your_token_here
set MODEL_NAME=username/custom-model  # Optional
```

**Docker Run (cross-platform):**  

```
docker run -it --rm -p 8000:8000 -p 7860:7860 -e HF_TOKEN=your_token_here neurodivergenthelper
```

> ðŸ’¡ Tip: On Windows PowerShell, using `-e HF_TOKEN=$env:HF_TOKEN` in Docker ensures the token passes into the container.

---

## CPU Model Recommendations

| Model Name                  | Description                              | CPU Performance |
| --------------------------- | ---------------------------------------- | --------------- |
| `microsoft/DialoGPT-small`  | Optimized for conversational flow        | Recommended     |
| `EleutherAI/gpt-neo-125M`   | Small GPT-Neo variant, fast on CPU       | Good            |
| `distilgpt2`                 | Very small GPT-2 variant, minimal memory | Basic/Fast      |
| `microsoft/DialoGPT-medium` | Larger, higher-quality model, slower     | Fallback        |
| `MODEL_NAME` (env var)       | Custom user-defined Hugging Face model   | Depends on size |

---

## Running with Docker Desktop (CPU)

1. Clone repository:

```
git clone https://github.com/NeurosynLabs/NeurodivergentHelper.git C:\NeurodivergentHelper
cd C:\NeurodivergentHelper
```

2. (Optional) Override `app.py` or `Dockerfile`:

```
Copy-Item "C:\Overrides\app.py" -Destination ".\app.py" -Force
Copy-Item "C:\Overrides\Dockerfile" -Destination ".\Dockerfile" -Force
```

3. Build Docker image:

```
docker build -t neurodivergenthelper .
```

4. Run container:

```
docker run -it --rm -p 8000:8000 -p 7860:7860 -e HF_TOKEN=your_token_here neurodivergenthelper
```

**Access:**

* FastAPI + Embeddable Chat: `http://localhost:7860`  
* API Documentation: `http://localhost:7860/docs`  
* Embed Interface: `http://localhost:7860/embed`

---

## Quick One-Line Docker Run

```
if (Test-Path "C:\NeurodivergentHelper") { cd C:\NeurodivergentHelper; git reset --hard; git clean -fd; git pull origin main } else { git clone https://github.com/NeurosynLabs/NeurodivergentHelper.git C:\NeurodivergentHelper; cd C:\NeurodivergentHelper }; docker build -t neurodivergenthelper .; docker run -it --rm -p 7860:7860 -e HF_TOKEN=your_token_here neurodivergenthelper
```

> Access `http://localhost:7860/embed` for the embeddable chat interface.

---

## Getting Started

### 1. Install dependencies

```
pip install -r requirements.txt
```

### 2. Set Hugging Face token (Linux/macOS/WSL)

```
export HF_TOKEN="your_token_here"
export MODEL_NAME="microsoft/DialoGPT-small"  # Optional
```

### 3. Set Hugging Face token (Windows PowerShell)

```
$env:HF_TOKEN="your_token_here"
$env:MODEL_NAME="microsoft/DialoGPT-small"  # Optional
```

### 4. Set Hugging Face token (Windows CMD)

```
set HF_TOKEN=your_token_here
set MODEL_NAME=microsoft/DialoGPT-small  # Optional
```

### 5. Run Locally (Python)

```
python app.py
```

> Access the interface at `http://localhost:7860`, embed at `/embed`, and API docs at `/docs`.

---

## Troubleshooting

* `RuntimeError: No CPU models could be loaded!` â†’ Ensure `HF_TOKEN` is set and model is available.  
* `ModuleNotFoundError: No module named 'yaml'` â†’ Install `pyyaml`.  
* Slow responses â†’ Use smaller models (`distilgpt2`) or increase CPU threads.  
* CORS errors in iframe â†’ Check CORS middleware in `app.py`.  
* Gradio port conflicts â†’ Adjust `server_port` in `app.py`.  

**CPU Thread Optimization:**

```
export OMP_NUM_THREADS=4
export MKL_NUM_THREADS=4
export OPENBLAS_NUM_THREADS=4
```

> Adjust thread counts based on your CPU cores for optimal performance.

---

## Development Notes

**Recent Enhancements:**

* YAML prompt loading from GitHub  
* CPU performance optimizations  
* Built-in embeddable chat interface (`/embed`)  
* Session management with memory cleanup  
* CORS support for iframe embedding  
* Better error handling and fallbacks  
* Added `models.yml` for optional multi-model configuration  
* Added `style.css` for custom interface styling  
* Added `utils.py` for session and helper functions  

**Planned Features:**

* Redis session storage  
* Model caching and warm-up  
* Rate limiting and authentication  
* Advanced prompt templating  
* GPU-optimized Docker container (Linux only)  
* Persistent session storage with database backend

---

## Feedback & Contribution

* Email: [neurosynlabs@proton.me](mailto:neurosynlabs@proton.me), [NeurosynLabs@google.com](mailto:NeurosynLabs@google.com)  
* Website: [https://neurodivergentexperiences.com](https://neurodivergentexperiences.com)

---

## License

**Proprietary â€“ All Rights Reserved**

```
Copyright (c) 2025 NeurosynLabs. All rights reserved.
This software and its associated files are proprietary.
No part may be reproduced, modified, distributed, or sold without written permission.
```

* Free CPU version for local testing only.  
* Paid version features and modules restricted via tokens and environment variables.

---

## Developer Info

**Developer:** Jarred Gainer  
**Emails:** [neurosynlabs@proton.me](mailto:neurosynlabs@proton.me), [NeurosynLabs@google.com](mailto:NeurosynLabs@google.com)
